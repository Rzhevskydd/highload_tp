# highload_tp

Project of "highload" course in technopark

# HighloadArchitectureCourseWork

## 1. Тема и целевая аудитория

В качестве темы был выбран сервис облачного хранилища данных. 
Прототипы:

- [Dropbox](https://www.dropbox.com/) 
- [Yandex.Disk](https://disk.yandex.ru/)
- [Google Drive](https://drive.google.com/)

Проектируемый функционал MVP:

* загрузить файл
* удалить файл
* скачать файл
* посмотреть информацию о файле
* поделиться ссылкой на файл

Положим размер аудитории сервиса 10М, расположение аудитории - Россия.

[Сводная табличка по аналогичным сервисам](https://en.wikipedia.org/wiki/Comparison_of_file_hosting_services)

## 2. Расчет нагрузки

Сервис будет предоставлять **2гб** дискового пространства для бесплатного аккаунта, **1000гб** для платного.
Ограничим объем загружаемых и скачиваемых данных для пользователя за день размером в **20гб/день** для бесплатного аккаунта, **200гб/день** для платного.
Также положим, что **99%** пользователей сервиса имеют бесплатный аккаунт, а **1%** будут иметь платную подписку, т.е. платный аккаунт.
Исходя из этого, получем следующие данные:

| account type | storage | percentage | bandwidth limit | total storage size               |
| ------------ | ------- | ---------- | --------------- | -------------------------------- |
| free         | 2GB     | 99%        | 4GB/day         | 2GB * 0.99 * 10 * 10^6 =  19.8PB |
| paid         | 1000GB  | 1%         | 200GB/day       | 1000GB * 0.01 * 10 * 10^6 = 10PB |

Для расчета нагрузки будем рассматривать один из самых популярных случаев использования файловых хостингов - загрузку фотографий/документов небольшого размера и синхронизацию файлов по устройствам пользователя.

Примем ряд некоторых допущений:

* Примем средний размер хранимых файлов **4МБ** 

* Примем входящий и исходящий траффик одинаковым по объему (в первоначальном приближении)

* Бесплатные пользователи почти полностью утилизируют объём доступного им хранилища (**2ГБ**)

* Пиковый трафик будет примерно в **3** раза больше, чем средний по рассчету

  [similarweb](https://www.similarweb.com/website/disk.yandex.ru/)

| Параметр                                                     | Значение |
| :----------------------------------------------------------- | -------- |
| Аудитория                                                    | 10М      |
| Количество пользователей в сутки                             | 100 000  |
| Количество загружаемых/скачиваемых пользователем файлов в день | 200      |
| Средний размер одного файла                                  | 4 МБ     |

### Сетевой трафик:

Поправочный коэф. - 2

10e5 * 200 * 4МБ * 2 = 80 000 ГБ/день = **14800 Мбит/сек**

В период пиковой нагрузки **~45000 Мбит/сек**

### Объем хранилища:

Суммарных объем хранилища  19.8PB + 10PB = **29.8PB**.


## 3. Логическая схема БД

![](./assets/logic_scheme.png)



Ориентировочно:
Для хранения данных пользователей будем использовать PostgreSQL, т.к. в ней доступно настраивается шардирование, таблицы User будут шардироваться по id.

Таблицы  Block будут шардироваться по полю id, каждый инстанс внутри шарда будет партиционироваться с целью облегчить вес таблицы и вес индексов по file_id. 
Memcached для отдачи физических путей расположения серверов у недавно использованных файлов. 



Для проверки целостности данных блоков хранятся хеши от их содержимого, вычисляемые на клиенте.

Координатор проксирует пользователей в нужный датацентр по критериям географического расположения и наличию платной подписки (в этом случае запросы идут на более мощные сервера большей пропускной способностью) 

Упрощенный схема хранения

![](./assets/simple_scheme.png)



## 4. Физическая схема



![](./assets/physic_scheme.png)



## 5. Оборудование

#### Расчет требуемого количества памяти

Учитывая, что размер хранилища у нас 29.8ПБ, то количество файлов будет около 29.8 * 10^9 МБ / 4 МБ = **7.45 * 10^9.**

В секунду грузится порядка 500 файлов.

Users: (8 + 64 + 64 + 8 + 1) * 10 * 10^6 = 145 Б * 10 * 10^6 =  1470 МБ

Files: (8+64+8+8+8+8+8+8+1+1) * 7.45 * 10^9 = 122 Б * 7.45 * 10^9 = 900 ГБ

Blocks: (8+64+8+8+32)  * 7.45 * 10^9 =  120 Б * 7.45 * 10^9 = 894 ГБ

Суммарно, для хранения метаинформации потребуется около ~2ТБ дискового пространства.

Для организации **хранения метаданных** возьмем 6 SSD по 2ТБ в RAID10 и это даст двухкратное ускорение записи и четырёхкратное ускорение чтения, итоговый размер будет 6ТБ. Также для надёжности будем использовать ещё 2 slave реплики.

Для организации **блочного хранилища** возьмем диски WD по 20ТБ и стенд от WD c 68 дисками (следовательно в сервер помещаетя 1360ТБ), большим объемом ОЗУ и двумя мощными процессорами Intel XEON. Данные будем хранить с использованием RAID10, соответственно каждый такой стенд даёт 680ТБ дискового пространства, а использование RAID10 с таким объемом даёт увеличение скорости чтения в 68 раз, а записи в 34 раза по сравнению с одинарным диском.

Для покрытия 29.8 ПБ данных пользователей требуется 29.8 ПБ  / 0.68ПБ = 44 стенда + 2 реплики, итого 132 стенда.

| Server                  | CPU                 | RAM               | Disk Type | RAID Type | Disks count | Storage Size with RAID | Amount               | Replicas count | Total Amount |
| ----------------------- | ------------------- | ----------------- | --------- | --------- | ----------- | ---------------------- | -------------------- | -------------- | ------------ |
| nginx                   | 32                  | 128GB             | 512GB SDD | -         | 1           | -                      | 2 & 2 in reserve = 4 | 2              | 12           |
| Redis (ссылки + сессии) | 8                   | 16GB              | 1TB SDD   | -         | 1           | -                      | 1                    | 2              | 3            |
| Metaserver              | 24                  | 32GB              | 2TB SSD   | RAID10    | 6           | 6TB                    | 1                    | 2              | 3            |
| Blockserver             | 24 + 24 (dual core) | 32GB * 16 = 512GB | 20TB HDD  | RAID10    | 68          | 680TB                  | 44                   | 2              | 132          |
|                         |                     |                   |           |           |             |                        |                      |                |              |
